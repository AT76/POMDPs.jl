{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiger Tutorial: Solving POMDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial outlines how to define a POMDP using the [POMDPs.jl](https://github.com/sisl/POMDPs.jl) interface. We will go through a simple problem simply known as the tiger problem (we will refer to it as the tiger POMDP). After defining the tiger POMDP, we will use QMDP and SARSOP to solve the POMDP. If you are new to working with this package, check out the [tutorial](http://nbviewer.ipython.org/github/sisl/POMDPs.jl/blob/master/examples/GridWorld.ipynb) on MDPs first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Overview\n",
    "In the tiger POMDP, the agent is tasked with escaping from a room. There are two doors leading out of the room. Behind one of the doors is a tiger, and behind the other is sweet, sweet freedom. If the agent opens the door and finds the tiger, it gets eaten (and receives a reward of -100). If the agent opens the other door, it escapes and receives a reward of 10. The agent can also listen. Listening gives a noisy measuremnt of which door the tiger is hiding behind. Listening gives the agent the correct location of the tiger 85% of the time. The agent receives a reward of -1 for listening. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first import POMDPs.jl\n",
    "using POMDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POMDP \n",
    "A POMDP is defined by the tuple\n",
    "$$(\\mathcal{S}, \\mathcal{A}, \\mathcal{Z}, T, R, O).$$\n",
    "In addition to the familiar, state $\\mathcal{S}$ and action $\\mathcal{A}$ spaces, we must also define an observation space $\\mathcal{Z}$ and an observation function $O$. The POMDP problem definition may be similar to the one for MDP. For example, if you wanted to add state uncertaitniy to your problem, you can define the observation space, and observation function in addition to your previous MDP definition.\n",
    "\n",
    "Before defining the spaces for this problem, let's first deinfe the concrete type for the tiger POMDP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TigerPOMDP (constructor with 3 methods)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type TigerPOMDP <: POMDP\n",
    "    r_listen::Float64 # reward for listening (default -1)\n",
    "    r_findtiger::Float64 # reward for finding the tiger (default -100)\n",
    "    r_escapetiger::Float64 # reward for escaping (default 10)\n",
    "    p_listen_correctly::Float64 # prob of correctly listening (default 0.85)\n",
    "    discount_factor::Float64 # discount\n",
    "end\n",
    "# default constructor\n",
    "function TigerPOMDP()\n",
    "    return TigerPOMDP(-1.0, -100.0, 10.0, 0.85, 0.95)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of parameters in the problem definition, but we can treat them all as constants. You can read more about the Tiger problem and POMDPs [here](http://www.techfak.uni-bielefeld.de/~skopp/Lehre/STdKI_SS10/POMDP_tutorial.pdf#page=28). However, we created a default constructor that allows us to initialize the tiger POMDP by simply running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TigerPOMDP(-1.0,-100.0,10.0,0.85,0.95)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pomdp = TigerPOMDP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## States\n",
    "We define a concrete type to represent the state of the tiger POMDP. The only thing we need to know to represent the problem is which door the tiger is hiding behind. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type TigerState <: State\n",
    "    tigerleft::Bool\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the state is a binary value, we represent it as a boolean, but we could have represented it as an integer or any other sensible type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions\n",
    "There are three possible actions our agent can take: open the left door, open the right door, and listen. For clarity, we will represent these with symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type TigerAction <: Action\n",
    "    act::Symbol\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will represent our actions with the following symbols: open left (:openl), open right (:openr), and listen (:listen). For example, the action below represnts listening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TigerAction(:listen)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = TigerAction(:listen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "There are two possible observations: the agent either hears the tiger behind the left door or behind the right door. We use a boolean to represent the observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type TigerObservation <: Observation\n",
    "    obsleft::Bool \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spaces\n",
    "Let's define our state, action and observation spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Space\n",
    "There are only two states in the tiger POMDP: the tiger is either behind the left door or behind the right door. Our state space is simply an array of the states in the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type TigerStateSpace <: AbstractSpace\n",
    "    states::Vector{TigerState} \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the ```states``` and the ```domain``` functions. Recall, that the ```states``` function returns the state space for a given POMDP type, and the ```domain``` function returns an iterator for a given space. Here, the domain function returns an array of the two possible states in our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "POMDPs.states(::TigerPOMDP) = TigerStateSpace([TigerState(true), TigerState(false)])\n",
    "POMDPs.domain(space::TigerStateSpace) = space.states;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in the [tutorial](http://nbviewer.ipython.org/github/sisl/POMDPs.jl/blob/master/examples/GridWorld.ipynb) on MDPs, we also defined a ```rand!``` function that sampled the space. We do not need this function when using QMDP or SARSOP, so we do not define it here. However, if you wanted to use Monte Carlo solvers solvers like POMCP or DESPOT you would need a function that can sample your spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Space\n",
    "There are three actions in our problem. Once again, we represent the action space as an array of the actions in our problem. The ```actions``` and ```domain``` functions serve a similar purpose to the ```states``` and ```domain``` functions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type TigerActionSpace <: AbstractSpace\n",
    "    actions::Vector{TigerAction} \n",
    "end\n",
    "# define actions function\n",
    "POMDPs.actions(::TigerPOMDP) = TigerActionSpace([TigerAction(:openl), TigerAction(:openr), TigerAction(:listen)]); # default\n",
    "POMDPs.actions(::TigerPOMDP, ::TigerState, acts::TigerActionSpace) = acts; # convenience (actions do not change in different states)\n",
    "# define domain function\n",
    "POMDPs.domain(space::TigerActionSpace) = space.actions;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation Space\n",
    "The observation space looks similar to the state space. Recall that the state represents the truth about our system, while the observation is potentially untuthful information recieves about the state. In the tiger POMDP, our observation could give us a false representation of our state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type TigerObservationSpace <: AbstractSpace\n",
    "    obs::Vector{TigerObservation} \n",
    "end\n",
    "# function returning observation space\n",
    "POMDPs.observations(::TigerPOMDP) = TigerObservationSpace([TigerObservation(true), TigerObservation(false)]);\n",
    "POMDPs.observations(::TigerPOMDP, s::TigerState, obs::TigerObservationSpace) = obs;\n",
    "# function returning an iterator over that space\n",
    "POMDPs.domain(space::TigerObservationSpace) = space.obs;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've defined the POMDP spaces, let's move on to defining the model functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition and Observation Distributions\n",
    "Before defining the model functions, we first need to create a distributions data-type. In general, our distributions should support sampling and have a ```pdf``` method. Once again, since we will not be using any sampling based solvers, we don't have to worry about implementing any sampling functions, so we only need to write a function that gives us the pdf.\n",
    "\n",
    "Since the transition and observation distributions have identical form, we could just use a single type to serve the needs of both. This will not be the case in general, and we will define two seperate types (although similar looking) for clarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transition distribution type\n",
    "type TigerTransitionDistribution <: AbstractDistribution\n",
    "    probs::Vector{Float64} \n",
    "end\n",
    "# transition distribution initializer\n",
    "POMDPs.create_transition_distribution(::TigerPOMDP) = TigerTransitionDistribution([0.5, 0.5])\n",
    "\n",
    "# observation distribution type\n",
    "type TigerObservationDistribution <: AbstractDistribution\n",
    "    probs::Vector{Float64} \n",
    "end\n",
    "# observation distribution initializer\n",
    "POMDPs.create_observation_distribution(::TigerPOMDP) = TigerObservationDistribution([0.5, 0.5]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define the ```pdf``` function. For a discrete problem, this function returns the probability mass of a given element (state or observation in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transition pdf\n",
    "function POMDPs.pdf(d::TigerTransitionDistribution, s::TigerState)\n",
    "    s.tigerleft ? (return d.probs[1]) : (return d.probs[2]) \n",
    "end;\n",
    "# obsevation pdf\n",
    "function POMDPs.pdf(d::TigerObservationDistribution, o::TigerObservation)\n",
    "    o.obsleft ? (return d.probs[1]) : (return d.probs[2])\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all we have to do for our distribution functions. Remeber that the tiger POMDP is a fairly simple problem, and you will usually need more expressive representations for your distirubtions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transition Model\n",
    "Here we define the transition model for the tiger POMDP. The model itself is fairly simple. Our state is represented by the location of the tiger (left or right). The location of the tiger doesn't change when the agent listens. However, after the agent opens the door, it reaches a terminal state. That is the agent either escapes or gets eaten. To simplify our formulation, we simply reset the location of the tiger randomly. We could model this problem with a terminal state (i.e. one in which the agent no longer receives reward) as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the transition mode\n",
    "function POMDPs.transition(pomdp::TigerPOMDP, s::TigerState, a::TigerAction, d::TigerTransitionDistribution=create_transition_distribution(pomdp))\n",
    "    probs = d.probs\n",
    "    # if open a door reset the tiger probs\n",
    "    if a.act == :openl || a.act == :openr\n",
    "        fill!(probs, 0.5)\n",
    "    # if tiger is on the left, distribution = 1.0 over the first state\n",
    "    elseif s.tigerleft\n",
    "        probs[1] = 1.0\n",
    "        probs[2] = 0.0\n",
    "    # otherwise distribution = 1.0 over the second state\n",
    "    else\n",
    "        probs[1] = 0.0\n",
    "        probs[2] = 1.0\n",
    "    end\n",
    "    d\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Model\n",
    "The reward model caputres the immediate objectives of the agent. It recieves a large negative reward for opening the door with the tiger behind it (-100), gets a positive reward for opening the other door (+10), and a small penalty for listening (-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function POMDPs.reward(pomdp::TigerPOMDP, s::TigerState, a::TigerAction)\n",
    "    r = 0.0\n",
    "    # small penalty for listening\n",
    "    if a.act == :listen\n",
    "        r += pomdp.r_listen\n",
    "    end\n",
    "    if a.act == :openl\n",
    "        # find tiger behind left door\n",
    "        if s.tigerleft\n",
    "            r += pomdp.r_findtiger\n",
    "        # escape through left door\n",
    "        else\n",
    "            r += pomdp.r_escapetiger\n",
    "        end\n",
    "    end\n",
    "    if a.act == :openr\n",
    "        # escape through the right door\n",
    "        if s.tigerleft\n",
    "            r += pomdp.r_escapetiger\n",
    "            # find tiger behind the right door\n",
    "        else\n",
    "            r += pomdp.r_findtiger\n",
    "        end\n",
    "    end\n",
    "    return r\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Model\n",
    "The observation model captures the uncertaintiy in the agent's lsitening ability. When we listen, we receive a noisy measurement of the tiger's location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function POMDPs.observation(pomdp::TigerPOMDP, s::TigerState, a::TigerAction, d::TigerObservationDistribution=create_observation_distribution(pomdp))\n",
    "    probs = d.probs\n",
    "    p = pomdp.p_listen_correctly # probability of listening correctly\n",
    "    if a.act == :listen\n",
    "        # if tiger is behind left door\n",
    "        if s.tigerleft\n",
    "            probs[1] = p # correct prob\n",
    "            probs[2] = (1.0-p) # wring prob\n",
    "        # if tiger is behind right door\n",
    "        else\n",
    "            probs[1] = (1.0-p) # wrong prob\n",
    "            probs[2] = p # correct prob\n",
    "        end\n",
    "    # if don't listen uniform\n",
    "    else\n",
    "        fill!(probs, 0.5)\n",
    "    end\n",
    "    d\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscallenous Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the ```discount``` function and the functions that return the size of our spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "POMDPs.discount(pomdp::TigerPOMDP) = pomdp.discount_factor\n",
    "POMDPs.n_states(::TigerPOMDP) = 2\n",
    "POMDPs.n_actions(::TigerPOMDP) = 3\n",
    "POMDPs.n_observations(::TigerPOMDP) = 2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beliefs\n",
    "If you are somewhat familiar with Julia defining all of the above may have been relaitvely simple. However, all POMDPs must be represented with a belief. Implementing beliefs and their updaters can be tricky. Luckily, we provide you with some nice tools to work with beliefs. Note that if you just want to use SASROP to solve for the alpha-vectors, and use your own belief updating scheme you do not need to implement the functions below.\n",
    "\n",
    "We will use the [POMDPToolbox](https://github.com/sisl/POMDPToolbox.jl) module which implements a discrete belief type and a belief udpater for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we will use the POMDPToolbox module\n",
    "using POMDPToolbox\n",
    "\n",
    "# define a create b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading the model ...\n",
      "  input file   : tiger.pomdpx\n",
      "  loading time : 0.00s \n",
      "\n",
      "SARSOP initializing ...\n",
      "  initialization time : 0.00s\n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      " Time   |#Trial |#Backup |LBound    |UBound    |Precision  |#Alphas |#Beliefs  \n",
      "-------------------------------------------------------------------------------\n",
      " 0       0       0        -20        92.8206    112.821     3        1        \n",
      " 0       2       51       -6.2981    63.1396    69.4377     7        16       \n",
      " 0       4       103      0.149651   52.2764    52.1268     9        21       \n",
      " 0       6       151      6.19248    42.0546    35.8621     9        21       \n",
      " 0       8       200      10.3563    35.232     24.8757     12       21       \n",
      " 0       11      250      14.0433    29.5471    15.5037     6        21       \n",
      " 0.01    14      300      16.545     25.0926    8.54759     10       21       \n",
      " 0.01    17      350      18.2281    21.8163    3.5882      14       21       \n",
      " 0.01    18      400      18.7451    20.9384    2.19328     8        21       \n",
      " 0.01    21      465      19.1109    20.0218    0.910956    5        21       \n",
      " 0.01    22      500      19.2369    19.7071    0.470219    11       21       \n",
      " 0.01    24      550      19.3036    19.5405    0.236865    6        21       \n",
      " 0.01    25      600      19.3369    19.4574    0.120445    13       21       \n",
      " 0.01    27      669      19.3579    19.4049    0.0469305   5        21       \n",
      " 0.01    28      713      19.3643    19.389     0.024739    5        21       \n",
      " 0.01    29      757      19.3676    19.3807    0.0130409   5        21       \n",
      " 0.01    30      801      19.3694    19.3763    0.0068744   5        21       \n",
      " 0.01    31      850      19.3704    19.3739    0.00351433  10       21       \n",
      " 0.01    32      900      19.3709    19.3725    0.00155165  5        21       \n",
      " 0.01    33      936      19.3711    19.3721    0.000976551 8        21       \n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "SARSOP finishing ...\n",
      "  target precision reached\n",
      "  target precision  : 0.001000\n",
      "  precision reached : 0.000977 \n",
      "\n",
      "-------------------------------------------------------------------------------\n",
      " Time   |#Trial |#Backup |LBound    |UBound    |Precision  |#Alphas |#Beliefs  \n",
      "-------------------------------------------------------------------------------\n",
      " 0.01    33      936      19.3711    19.3721    0.000976551 5        21       \n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "Writing out policy ...\n",
      "  output file : tiger.policy\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "POMDPAlphas(2x5 Array{Float64,2}:\n",
       " -81.5975   3.01448  24.6954    28.4025  19.3711\n",
       "  28.4025  24.6954    3.01452  -81.5975  19.3711,[0,2,2,1,2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using SARSOP\n",
    "pomdp = TigerPOMDP()\n",
    "policy = POMDPPolicy(\"tiger.policy\")\n",
    "pomdpfile = POMDPFile(pomdp, \"tiger.pomdpx\")\n",
    "solver = SARSOPSolver()\n",
    "solve(solver, pomdpfile, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " -81.5975\n",
       "  28.4025"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas(policy)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.3.4",
   "language": "julia",
   "name": "julia-0.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
