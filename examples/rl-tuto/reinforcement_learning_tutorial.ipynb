{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Tutorial for POMDPs.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspired from CS234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using POMDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Reinforcement Learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# POMDPs.add(\"TabularTDLearning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using TabularTDLearning, POMDPToolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wall-E exploration\n",
    "\n",
    "\n",
    "Let's consider a 1D gridworld and 2 different rewards at each extremities. Wall-E must find the plant but he might greedily join his lover instead...\n",
    "\n",
    "![wall-e-mdp](initial_state.png)\n",
    "\n",
    "The environment can be modeled as follow\n",
    "\n",
    "- 10 states: 1,2,3,4,5,6,7,8,9,10\n",
    "- 2 actions: left, right\n",
    "- one episode lasts until a reward is found\n",
    "- there is a reward of +1 in state 1 (Eve) and +2 in state 10 (the plant)\n",
    "- Wall-E starts at 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mutable struct MarsExp <: MDP{Int64, Symbol}\n",
    "    r_left::Float64\n",
    "    r_right::Float64\n",
    "    start::Int64\n",
    "    γ::Float64\n",
    "    MarsExp(;r_left::Float64 = 1., r_right::Float64 = 2.,start::Int64 = 5, γ::Float64 = 0.9) = new(r_left, r_right, start, γ)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mMethodError: no method matching requirements_info(::Type{TabularTDLearning.QLearningSolver}, ::MarsExp)\u001b[0m\nClosest candidates are:\n  requirements_info(\u001b[91m::Union{POMDPs.Simulator, POMDPs.Solver}\u001b[39m, ::Union{POMDPs.MDP, POMDPs.POMDP}, \u001b[91m::Any...\u001b[39m) at C:\\Users\\Maxime\\.julia\\v0.6\\POMDPs\\src\\requirements_interface.jl:140\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mMethodError: no method matching requirements_info(::Type{TabularTDLearning.QLearningSolver}, ::MarsExp)\u001b[0m\nClosest candidates are:\n  requirements_info(\u001b[91m::Union{POMDPs.Simulator, POMDPs.Solver}\u001b[39m, ::Union{POMDPs.MDP, POMDPs.POMDP}, \u001b[91m::Any...\u001b[39m) at C:\\Users\\Maxime\\.julia\\v0.6\\POMDPs\\src\\requirements_interface.jl:140\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m.\\loading.jl:515\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "@requirements_info QLearningSolver MarsExp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function POMDPs.states(mdp::MarsExp)\n",
    "    return 1:1:10\n",
    "end\n",
    "POMDPs.state_index(mdp::MarsExp, s::Int64) = s\n",
    "POMDPs.n_states(mdp::MarsExp) = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function POMDPs.actions(mdp::MarsExp)\n",
    "    return [:left, :right]\n",
    "end\n",
    "POMDPs.action_index(mdp::MarsExp, a::Symbol) = a == :left ? 1 : 2\n",
    "POMDPs.n_actions(mdp::MarsExp) = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function POMDPs.generate_s(mdp::MarsExp, s::Int64, a::Symbol, rng::AbstractRNG)\n",
    "    if a == :left\n",
    "        return max(1, s-1)\n",
    "    elseif a == :right\n",
    "        return min(10, s+1)\n",
    "    end\n",
    "end\n",
    "function POMDPs.reward(mdp::MarsExp, s::Int64, a::Symbol, sp::Int64)\n",
    "    if sp == 1\n",
    "        return mdp.r_left\n",
    "    elseif sp == 10\n",
    "        return mdp.r_right\n",
    "    else\n",
    "        return 0.0\n",
    "    end\n",
    "end     \n",
    "function POMDPs.initial_state(mdp::MarsExp, rng::AbstractRNG)\n",
    "    return mdp.start\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function POMDPs.isterminal(mdp::MarsExp, s::Int64)\n",
    "    return s == 1 || s == 10\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve with Q-learning\n",
    "\n",
    "First we need to initialize the solver with the desired hyper parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO: POMDPs.jl requirements for \u001b[34msolve(::QLearningSolver, ::Union{POMDPs.MDP,POMDPs.POMDP})\u001b[39m and dependencies. ([✔] = implemented correctly; [X] = missing)\n",
      "\n",
      "For \u001b[34msolve(::QLearningSolver, ::Union{POMDPs.MDP,POMDPs.POMDP})\u001b[39m:\n",
      "\u001b[32m  [✔] initial_state(::MarsExp, ::AbstractRNG)\u001b[39m\n",
      "\u001b[32m  [✔] generate_sr(::MarsExp, ::Int64, ::Symbol, ::AbstractRNG)\u001b[39m\n",
      "\u001b[32m  [✔] state_index(::MarsExp, ::Int64)\u001b[39m\n",
      "\u001b[32m  [✔] action_index(::MarsExp, ::Symbol)\u001b[39m\n",
      "\u001b[32m  [✔] discount(::MarsExp)\u001b[39m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@requirements_info QLearningSolver(MarsExp()) MarsExp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then initialize the problem and the solver using the desired hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = MarsExp(start=2)\n",
    "solver = QLearningSolver(mdp, learning_rate=0.1, n_episodes=400, max_episode_length=50, eval_every=50, n_eval_traj=100, \n",
    "                         exp_policy=EpsGreedyPolicy(mdp, 0.9));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to solve for the optimal policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Iteration 50, Returns: 1.0\n",
      "On Iteration 100, Returns: 1.0\n",
      "On Iteration 150, Returns: 1.0\n",
      "On Iteration 200, Returns: 1.0\n",
      "On Iteration 250, Returns: 1.0\n",
      "On Iteration 300, Returns: 1.0\n",
      "On Iteration 350, Returns: 1.0\n",
      "On Iteration 400, Returns: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "POMDPToolbox.ValuePolicy{Any}(MarsExp(1.0, 2.0, 2, 0.9), [0.0 0.0; 1.0 1.0; … ; 0.797659 1.43514; 0.0 0.0], Any[:left, :right])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = solve(solver, mdp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "include(\"render.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POMDPToolbox.MDPHistory{Int64,Symbol}([2, 1], Symbol[:left], [1.0], 1.0, Nullable{Exception}(), Nullable{Any}())"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = MersenneTwister(3)\n",
    "hist = HistoryRecorder(max_steps=1000)\n",
    "\n",
    "hist = simulate(hist, mdp, policy, initial_state(mdp, rng))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some interesting experiments\n",
    "\n",
    "If you reduce exploration, you end up in a suboptimal policy.\n",
    "\n",
    "If the discount factor is too low then you end up with a greedier policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
