{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Tutorial for POMDPs.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspired from CS234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using POMDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Reinforcement Learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# POMDPs.add(\"TabularTDLearning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using TabularTDLearning, POMDPToolbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wall-E exploration\n",
    "\n",
    "\n",
    "Let's consider a 1D gridworld and 2 different rewards at each extremities. Wall-E must find the plant but he might greedily join his lover instead...\n",
    "\n",
    "![wall-e-mdp](initial_state.png)\n",
    "\n",
    "The environment can be modeled as follow\n",
    "\n",
    "- 10 states: 1,2,3,4,5,6,7,8,9,10\n",
    "- 2 actions: left, right\n",
    "- one episode lasts until a reward is found\n",
    "- there is a reward of +1 in state 1 (Eve) and +2 in state 10 (the plant)\n",
    "- Wall-E starts at 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mutable struct MarsExp <: MDP{Int64, Symbol}\n",
    "    r_left::Float64\n",
    "    r_right::Float64\n",
    "    start::Int64\n",
    "    γ::Float64\n",
    "    MarsExp(;r_left::Float64 = 1., r_right::Float64 = 10.,start::Int64 = 5, γ::Float64 = 0.99) = new(r_left, r_right, start, γ)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mMethodError: no method matching requirements_info(::Type{TabularTDLearning.QLearningSolver}, ::MarsExp)\u001b[0m\nClosest candidates are:\n  requirements_info(\u001b[91m::Union{POMDPs.Simulator, POMDPs.Solver}\u001b[39m, ::Union{POMDPs.MDP, POMDPs.POMDP}, \u001b[91m::Any...\u001b[39m) at C:\\Users\\Maxime\\.julia\\v0.6\\POMDPs\\src\\requirements_interface.jl:140\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mMethodError: no method matching requirements_info(::Type{TabularTDLearning.QLearningSolver}, ::MarsExp)\u001b[0m\nClosest candidates are:\n  requirements_info(\u001b[91m::Union{POMDPs.Simulator, POMDPs.Solver}\u001b[39m, ::Union{POMDPs.MDP, POMDPs.POMDP}, \u001b[91m::Any...\u001b[39m) at C:\\Users\\Maxime\\.julia\\v0.6\\POMDPs\\src\\requirements_interface.jl:140\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m.\\loading.jl:515\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "@requirements_info QLearningSolver MarsExp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function POMDPs.states(mdp::MarsExp)\n",
    "    return 1:1:10\n",
    "end\n",
    "POMDPs.state_index(mdp::MarsExp, s::Int64) = s\n",
    "POMDPs.n_states(mdp::MarsExp) = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function POMDPs.actions(mdp::MarsExp)\n",
    "    return [:left, :right]\n",
    "end\n",
    "POMDPs.action_index(mdp::MarsExp, a::Symbol) = a == :left ? 1 : 2\n",
    "POMDPs.n_actions(mdp::MarsExp) = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function POMDPs.generate_s(mdp::MarsExp, s::Int64, a::Symbol, rng::AbstractRNG)\n",
    "    if a == :left\n",
    "        return max(1, s-1)\n",
    "    elseif a == :right\n",
    "        return min(10, s+1)\n",
    "    end\n",
    "end\n",
    "function POMDPs.reward(mdp::MarsExp, s::Int64, a::Symbol, sp::Int64)\n",
    "    if sp == 1\n",
    "        return mdp.r_left\n",
    "    elseif sp == 10\n",
    "        return mdp.r_right\n",
    "    else\n",
    "        return 0.0\n",
    "    end\n",
    "end     \n",
    "function POMDPs.initial_state(mdp::MarsExp, rng::AbstractRNG)\n",
    "    return mdp.start \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function POMDPs.isterminal(mdp::MarsExp, s::Int64)\n",
    "    return s == 1 || s == 10\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve with Q-learning\n",
    "\n",
    "First we need to initialize the solver with the desired hyper parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO: POMDPs.jl requirements for \u001b[34msolve(::QLearningSolver, ::Union{POMDPs.MDP,POMDPs.POMDP})\u001b[39m and dependencies. ([✔] = implemented correctly; [X] = missing)\n",
      "\n",
      "For \u001b[34msolve(::QLearningSolver, ::Union{POMDPs.MDP,POMDPs.POMDP})\u001b[39m:\n",
      "\u001b[32m  [✔] initial_state(::MarsExp, ::AbstractRNG)\u001b[39m\n",
      "\u001b[32m  [✔] generate_sr(::MarsExp, ::Int64, ::Symbol, ::AbstractRNG)\u001b[39m\n",
      "\u001b[32m  [✔] state_index(::MarsExp, ::Int64)\u001b[39m\n",
      "\u001b[32m  [✔] action_index(::MarsExp, ::Symbol)\u001b[39m\n",
      "\u001b[32m  [✔] discount(::MarsExp)\u001b[39m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@requirements_info QLearningSolver(MarsExp()) MarsExp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then initialize the problem and the solver using the desired hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabularTDLearning.QLearningSolver(1000, 50, 0.1, POMDPToolbox.EpsGreedyPolicy(0.5, POMDPToolbox.ValuePolicy{Any}(MarsExp(1.0, 10.0, 4, 0.99), [0.0 0.0; 0.0 0.0; … ; 0.0 0.0; 0.0 0.0], Any[:left, :right]), POMDPToolbox.StochasticPolicy(MersenneTwister(UInt32[0xedad597d, 0x96c954c0, 0x5ac276a3, 0xc2ad1a2a], Base.dSFMT.DSFMT_state(Int32[-1522690906, 1073239195, 458614064, 1072943064, 658217498, 1073106907, 375654207, 1073190151, -807739292, 1073376892  …  388331957, 1073333209, 918934450, 1073169483, -44771095, -986091191, 85992859, -1875977699, 382, 0]), [1.52066, 1.23824, 1.3945, 1.47388, 1.65197, 1.22928, 1.4625, 1.54486, 1.9083, 1.51305  …  1.17703, 1.04775, 1.26268, 1.28971, 1.3476, 1.10172, 1.63457, 1.23743, 1.61031, 1.45417], 356), Symbol[:left, :right], MarsExp(1.0, 10.0, 4, 0.99), POMDPToolbox.VoidUpdater())), [0.0 0.0; 0.0 0.0; … ; 0.0 0.0; 0.0 0.0], 50, 100)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp = MarsExp(start=4)\n",
    "solver = QLearningSolver(mdp, learning_rate=0.1, n_episodes=1000, max_episode_length=50, eval_every=50, n_eval_traj=100, \n",
    "                         exp_policy = EpsGreedyPolicy(mdp, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to solve for the optimal policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Iteration 50, Returns: 1.0\n",
      "On Iteration 100, Returns: 1.0\n",
      "On Iteration 150, Returns: 1.0\n",
      "On Iteration 200, Returns: 1.0\n",
      "On Iteration 250, Returns: 1.0\n",
      "On Iteration 300, Returns: 1.0\n",
      "On Iteration 350, Returns: 1.0\n",
      "On Iteration 400, Returns: 1.0\n",
      "On Iteration 450, Returns: 1.0\n",
      "On Iteration 500, Returns: 1.0\n",
      "On Iteration 550, Returns: 1.0\n",
      "On Iteration 600, Returns: 1.0\n",
      "On Iteration 650, Returns: 1.0\n",
      "On Iteration 700, Returns: 1.0\n",
      "On Iteration 750, Returns: 1.0\n",
      "On Iteration 800, Returns: 1.0\n",
      "On Iteration 850, Returns: 1.0\n",
      "On Iteration 900, Returns: 1.0\n",
      "On Iteration 950, Returns: 1.0\n",
      "On Iteration 1000, Returns: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "POMDPToolbox.ValuePolicy{Any}(MarsExp(1.0, 10.0, 4, 0.99), [0.0 0.0; 1.0 1.0; … ; 0.0413447 0.0; 0.0 0.0], Any[:left, :right])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = solve(solver, mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×2 Array{Float64,2}:\n",
       "  0.0   0.0\n",
       "  1.0  10.0\n",
       " 10.0  10.0\n",
       " 10.0  10.0\n",
       " 10.0  10.0\n",
       " 10.0  10.0\n",
       " 10.0  10.0\n",
       " 10.0  10.0\n",
       " 10.0  10.0\n",
       "  0.0   0.0"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.value_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: redefining constant WALLE_IMG\n",
      "WARNING: redefining constant PLANT_IMG\n",
      "WARNING: redefining constant EVE_IMG\n"
     ]
    }
   ],
   "source": [
    "include(\"render.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POMDPToolbox.RandomPolicy{MersenneTwister,MarsExp,POMDPToolbox.VoidUpdater}(MersenneTwister(UInt32[0xedad597d, 0x96c954c0, 0x5ac276a3, 0xc2ad1a2a], Base.dSFMT.DSFMT_state(Int32[-1061178416, 1073167218, -2057272038, 1073641417, 133238069, 1072917503, -1778145389, 1073286043, 1801412150, 1073439155  …  1162524402, 1073260592, -966052076, 1072766075, -1961213945, 802650271, -494256477, -1906918512, 382, 0]), [1.45201, 1.90424, 1.21387, 1.56533, 1.71135, 1.06647, 1.54372, 1.64552, 1.143, 1.27429  …  1.49677, 1.09577, 1.12017, 1.49667, 1.12516, 1.15541, 1.825, 1.09416, 1.54106, 1.06945], 186), MarsExp(1.0, 10.0, 5, 0.9), POMDPToolbox.VoidUpdater())"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_policy = RandomPolicy(mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POMDPToolbox.MDPHistory{Int64,Symbol}([5, 6, 7, 6, 5, 4, 5, 6, 5, 4, 5, 4, 3, 2, 1], Symbol[:right, :right, :left, :left, :left, :right, :right, :left, :left, :right, :left, :left, :left, :left], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], 1.0, Nullable{Exception}(), Nullable{Any}())"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rng = MersenneTwister(3)\n",
    "hist = HistoryRecorder(max_steps=1000)\n",
    "\n",
    "hist = simulate(hist, mdp, random_policy, initial_state(mdp, rng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.79\n"
     ]
    }
   ],
   "source": [
    "random_perf = 0.\n",
    "for i=1:100\n",
    "    hist = HistoryRecorder(max_steps=1000)\n",
    "    hist = simulate(hist, mdp, random_policy, initial_state(mdp, rng))\n",
    "    random_perf += discounted_reward(hist)\n",
    "end\n",
    "println(random_perf/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some interesting experiments\n",
    "\n",
    "If you reduce exploration, you end up in a suboptimal policy.\n",
    "\n",
    "If the discount factor is too low then you end up with a greedier policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
